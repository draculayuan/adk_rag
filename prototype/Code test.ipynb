{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "155417a7-5abf-4f07-ac36-511fbe76fba6",
   "metadata": {},
   "source": [
    "### Prototying first - RAG + Agent SDK"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c514bc9-48b2-48d3-9006-a6e3ce0e3e7d",
   "metadata": {},
   "source": [
    "#### Utility functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "47c5a78c-3325-480e-b4c7-60975ca2fb8f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from typing import List, Dict, Any\n",
    "import PyPDF2\n",
    "from docx import Document\n",
    "import markdown\n",
    "import os\n",
    "import pandas as pd\n",
    "import pytesseract\n",
    "from PIL import Image\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "\n",
    "class DocumentProcessor:\n",
    "    def __init__(self):\n",
    "        #self.supported_types = settings.SUPPORTED_FILE_TYPES\n",
    "        self.text_splitter = RecursiveCharacterTextSplitter(\n",
    "            chunk_size=1000,\n",
    "            chunk_overlap=50,\n",
    "            length_function=len,\n",
    "            is_separator_regex=False,\n",
    "        )\n",
    "\n",
    "    def process_document(self, root_path: str) -> List[Dict[str, Any]]:\n",
    "        \"\"\"Process a document and return chunks with metadata.\"\"\"\n",
    "        if not os.path.exists(root_path):\n",
    "            raise FileNotFoundError(f\"File not found: {file_path}\")\n",
    "            \n",
    "        chunks_collection = []\n",
    "        for root, dirs, files in os.walk(file_root):\n",
    "            for file in files:\n",
    "                file_path = os.path.join(root, file)\n",
    "                file_ext = os.path.splitext(file_path)[1].lower()\n",
    "                #if file_ext not in self.supported_types:\n",
    "                    #raise ValueError(f\"Unsupported file type: {file_ext}\")\n",
    "\n",
    "                # Extract text based on file type\n",
    "                if file_ext == '.pdf':\n",
    "                    text = self._extract_pdf_text(file_path)\n",
    "                elif file_ext == '.docx':\n",
    "                    text = self._extract_docx_text(file_path)\n",
    "                elif file_ext == '.md':\n",
    "                    text = self._extract_markdown_text(file_path)\n",
    "                elif file_ext == '.csv':\n",
    "                    text = self._extract_csv(file_path)\n",
    "                elif file_ext in ['.png', 'jpeg', 'jpg']:\n",
    "                    text = self._extract_image(file_path)\n",
    "                else:  # .txt\n",
    "                    text = self._extract_text_file(file_path)\n",
    "\n",
    "                # Chunk the text\n",
    "                chunks = self._chunk_text(text)\n",
    "                chunks_collection.append(\n",
    "                    {\n",
    "                        \"chunks\": chunks,\n",
    "                        \"file_path\": file_path\n",
    "                    }\n",
    "                )\n",
    "\n",
    "        # Add metadata to chunks\n",
    "        return self._add_metadata(chunks_collection)\n",
    "\n",
    "    def _extract_csv(self, file_path: str) -> str:\n",
    "        texts = \"\"\n",
    "        df = pd.read_csv(file_path)\n",
    "        for i, row in df.iterrows():\n",
    "            text = \" | \".join([f\"{col}: {row[col]}\" for col in df.columns])\n",
    "            texts += '\\n ' + (text)\n",
    "        return texts\n",
    "            \n",
    "    def _extract_image(self, file_path: str) -> str:\n",
    "        img = Image.open(file_path)\n",
    "        text = pytesseract.image_to_string(img)\n",
    "        return text\n",
    "        \n",
    "    def _extract_pdf_text(self, file_path: str) -> str:\n",
    "        \"\"\"Extract text from PDF file.\"\"\"\n",
    "        text = \"\"\n",
    "        with open(file_path, 'rb') as file:\n",
    "            pdf_reader = PyPDF2.PdfReader(file)\n",
    "            for page in pdf_reader.pages:\n",
    "                text += page.extract_text() + \"\\n\"\n",
    "        return text\n",
    "\n",
    "    def _extract_docx_text(self, file_path: str) -> str:\n",
    "        \"\"\"Extract text from DOCX file.\"\"\"\n",
    "        doc = Document(file_path)\n",
    "        return \"\\n\".join([paragraph.text for paragraph in doc.paragraphs])\n",
    "\n",
    "    def _extract_markdown_text(self, file_path: str) -> str:\n",
    "        \"\"\"Extract text from Markdown file.\"\"\"\n",
    "        with open(file_path, 'r', encoding='utf-8') as file:\n",
    "            md_text = file.read()\n",
    "            return markdown.markdown(md_text)\n",
    "\n",
    "    def _extract_text_file(self, file_path: str) -> str:\n",
    "        \"\"\"Extract text from plain text file.\"\"\"\n",
    "        with open(file_path, 'r', encoding='utf-8') as file:\n",
    "            return file.read()\n",
    "\n",
    "    def _chunk_text(self, text: str) -> List[str]:\n",
    "        \"\"\"Split text into overlapping chunks.\"\"\"\n",
    "        chunks = self.text_splitter.split_text(text)\n",
    "\n",
    "        return chunks\n",
    "\n",
    "    def _add_metadata(self, chunks_collection: List[dict]) -> List[Dict[str, Any]]:\n",
    "        \"\"\"Add metadata to each chunk.\"\"\"\n",
    "        results = []\n",
    "        idx = 0\n",
    "        for data in chunks_collection:\n",
    "            chunks = data[\"chunks\"]\n",
    "            file_path = data[\"file_path\"]\n",
    "            file_name = os.path.basename(file_path)\n",
    "            for chunk in chunks:\n",
    "                results.append(\n",
    "                    {\n",
    "                        \"text\": chunk,\n",
    "                        \"metadata\": {\n",
    "                            \"source\": file_path,\n",
    "                            \"file_name\": file_name,\n",
    "                            \"chunk_index\": idx\n",
    "                        }\n",
    "                    }\n",
    "                )\n",
    "                idx += 1\n",
    "        return results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d8558eb4-4e11-4eb7-bd67-70805843ecf0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from typing import List, Dict, Any\n",
    "from google.cloud import aiplatform\n",
    "from vertexai.language_models import TextEmbeddingModel\n",
    "#from .config import settings\n",
    "\n",
    "class EmbeddingGenerator:\n",
    "    def __init__(self):\n",
    "        #self.project = settings.GOOGLE_CLOUD_PROJECT\n",
    "        #self.location = settings.VERTEX_AI_LOCATION\n",
    "        self.model = \"text-embedding-005\"\n",
    "        \n",
    "        \"\"\"\n",
    "        # Initialize Vertex AI\n",
    "        aiplatform.init(\n",
    "            project=self.project,\n",
    "            location=self.location\n",
    "        )\n",
    "        \"\"\"\n",
    "        \n",
    "        # Initialize the embedding model\n",
    "        self.embedding_model = TextEmbeddingModel.from_pretrained(self.model)\n",
    "\n",
    "    def generate_embeddings(self, chunks: List[Dict[str, Any]]) -> List[Dict[str, Any]]:\n",
    "        \"\"\"Generate embeddings for text chunks.\"\"\"\n",
    "        # Extract texts from chunks\n",
    "        texts = [chunk[\"text\"] for chunk in chunks]\n",
    "        \n",
    "        # Generate embeddings\n",
    "        embeddings = self.embedding_model.get_embeddings(texts)\n",
    "        \n",
    "        # Combine embeddings with original chunk data\n",
    "        for chunk, embedding in zip(chunks, embeddings):\n",
    "            chunk[\"embedding\"] = embedding.values\n",
    "            \n",
    "        return chunks\n",
    "\n",
    "    def generate_single_embedding(self, text: str) -> List[float]:\n",
    "        \"\"\"Generate embedding for a single text.\"\"\"\n",
    "        embedding = self.embedding_model.get_embeddings([text])[0]\n",
    "        return embedding.values "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8264117a-5d1e-49a7-b8a5-3378b3875c36",
   "metadata": {},
   "source": [
    "#### 1. Embedding management:\n",
    "\n",
    "Code that can:\n",
    "- Create index using multiple type of files \n",
    "- Create endpoint\n",
    "- Deploy index to endpoint \n",
    "- Query from endpoint "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "042c860a-3754-484c-8161-ebb605d01ff9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import faiss\n",
    "from __future__ import annotations\n",
    "from typing import List, Dict, Any, Optional\n",
    "from google.cloud import aiplatform\n",
    "from google.cloud.aiplatform.matching_engine import (\n",
    "    MatchingEngineIndex,\n",
    "    MatchingEngineIndexEndpoint,\n",
    ")\n",
    "import numpy as np\n",
    "#from src.common.config import settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5d007de-162b-4816-b6fc-e65b2e46f4e0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d462135c-4f58-42b4-9a7a-f55e81e5fc5c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Load tools\n",
    "processor = DocumentProcessor()\n",
    "embedder = EmbeddingGenerator()\n",
    "index = faiss.IndexFlatL2(768)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f59dda85-2de0-48fe-b316-900079b93acf",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# main process\n",
    "file_root = '/home/jupyter/code_test/adk_rag/prototype/test_data'\n",
    "\n",
    "# chunking \n",
    "processed_data = processor.process_document(file_root)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9b8cddb-724c-4631-9b74-47580a94b101",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# convert to embeddings\n",
    "with_embeddings = embedder.generate_embeddings(processed_data)\n",
    "# extract vectors \n",
    "vectors = np.array([e['embedding'] for e in with_embeddings])\n",
    "# build up index\n",
    "index.add(vectors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b699620e-bf17-46b0-81cc-e06497f2bd26",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# query process\n",
    "query = \"media effectiveness\"\n",
    "query_embeddings = np.array([embedder.generate_single_embedding(query)])\n",
    "_, I = index.search(query_embeddings, k=3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3c2d9b2-17a7-4a91-9139-e8887203e80a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "with_embeddings[I[0][0]]['text']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cbf55ee-2da1-465b-8a09-1888d16101f9",
   "metadata": {},
   "source": [
    "#### 2. Agent ADK \n",
    "\n",
    "Code that can:\n",
    "- Define an Agent\n",
    "- Deploy agent to Agent Engine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aef9bdaf-0507-47a7-b0d9-ffd42da5a021",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b5668ab-9ef8-4828-8b8b-70fe206c6ea4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from google import adk\n",
    "from google.adk.agents import LlmAgent\n",
    "from vertexai.preview.reasoning_engines import AdkApp\n",
    "from typing import List, Dict, Any\n",
    "from google.adk.tools import ToolContext\n",
    "from google.adk.sessions import InMemorySessionService"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ec34792-712e-459d-99e0-714d9a3e2a37",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def test_func(tool_context: ToolContext) -> str:\n",
    "    \"\"\"\n",
    "    just a test function that says yeah!\n",
    "\n",
    "    Returns:\n",
    "        yeah!.\n",
    "    \"\"\"\n",
    "    return \"yeah\" "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "938cdc7d-7c48-4dd4-a040-5d09049b5d4c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Define the RAG agent using ADK\n",
    "rag_agent = LlmAgent(\n",
    "    name=\"test_agent\",\n",
    "    model=\"gemini-2.0-flash\",\n",
    "    description=\"testing agent\",\n",
    "    instruction=(\n",
    "        \"Always use 'test_func()' tool when answering user questions\"\n",
    "    ),\n",
    "    #tools=[test_func],\n",
    "    \n",
    ")\n",
    "\n",
    "# Wrap the agent in an AdkApp for deployment\n",
    "app = AdkApp(agent=rag_agent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c0d2ed1-7aac-4d03-90d3-05afcaedecad",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "### try deploy to remote and interact with it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "117f8bbc-e131-4041-8faa-b74623011fe6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from vertexai import agent_engines\n",
    "import vertexai\n",
    "vertexai.init(staging_bucket=\"gs://yuan_evernote_rag_cs\")\n",
    "\n",
    "remote_app = agent_engines.create(\n",
    "    agent_engine=rag_agent,\n",
    "    requirements=[\n",
    "        \"google-cloud-aiplatform[adk,agent_engines]==1.90.0\"   \n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1b680e5-0553-48e8-9a87-09f5bbe758ed",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "test_agent = vertexai.agent_engines.get('projects/163097687798/locations/us-central1/reasoningEngines/2032750709153202176')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "003b507c-985d-4cc7-8f0f-371d94d509bb",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "session1 = test_agent.create_session(user_id=\"u_123\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c35150c-e890-418d-a5d0-78bb5532283b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Store the vector database in the session's state\n",
    "session1['state'][\"index\"] = index\n",
    "session1['state'][\"embedder\"] = embedder\n",
    "session1['state'][\"metadata_store\"] = with_embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b490683-443d-4bb0-bf2f-c00ff976284e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Create the ADK runner with VertexAiSessionService\n",
    "from google.adk.sessions import VertexAiSessionService\n",
    "session_service = VertexAiSessionService()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31a04b6b-7f95-4d74-9322-3803c922b616",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "runner = adk.Runner(\n",
    "    agent=test_agent,\n",
    "    app_name=\"test_app\",\n",
    "    session_service=session_service)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "312e94b3-5e1b-4f7d-995b-e43b524515f0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Helper method to send query to the runner\n",
    "session = session_service.create_session(app_name=\"test_app\", user_id=\"user123\")\n",
    "def call_agent(query, session_id, user_id):\n",
    "  content = types.Content(role='user', parts=[types.Part(text=query)])\n",
    "  events = runner.run(\n",
    "      user_id=user_id, session_id=session_id, new_message=content)\n",
    "\n",
    "  for event in events:\n",
    "      if event.is_final_response():\n",
    "          final_response = event.content.parts[0].text\n",
    "          print(\"Agent Response: \", final_response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f474c307-e993-41de-bf95-906938c0dba8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "call_agent(\"hi\", 123, \"u_123\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22b3eccc-8847-463b-90af-0365bb6ad2fd",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "for event in rag_agent.stream_query(\n",
    "    user_id=\"u_123\",\n",
    "    session_id=session1.id,\n",
    "    message=\"whatsup\",\n",
    "):\n",
    "    print(event)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12d69e68-bb84-4329-99d7-ce46f897206f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "test_agent.delete(force=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "408930e9-e5a8-4460-9979-b6f2f66c0794",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "def retrieve_documents(query, tool_context: ToolContext) -> List[str]:\n",
    "    \"\"\"\n",
    "    Vector-search tool: returns the top_k text snippets relevant to the question.\n",
    "\n",
    "    Args:\n",
    "        question: User query text.\n",
    "        tool_context: Cotext of the tool.\n",
    "    Returns:\n",
    "        List of document text snippets.\n",
    "    \"\"\"\n",
    "    index = tool_context.state.get(\"index\")\n",
    "    embedder = tool_context.state.get(\"embedder\")\n",
    "    metadata_store = tool_context.state.get(\"metadata_store\")\n",
    "    query_embeddings = np.array([embedder.generate_single_embedding(query)])\n",
    "    _, I = index.search(query_embeddings, k=5)\n",
    "    return [metadata_store[i]['text'] for i in I[0] if i >= 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7618b9b7-80bb-4af3-bd5c-ba6a510e3c63",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d22304bd-9f72-4f46-bea7-a626ac545330",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "# Create a session service and a new session\n",
    "session_service = InMemorySessionService()\n",
    "session = session_service.create_session(app_name=\"my_app\", user_id=\"user123\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "947ff4f0-97cc-409e-86a8-87bd30564846",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Store the vector database in the session's state\n",
    "session.state[\"index\"] = index\n",
    "session.state[\"embedder\"] = embedder\n",
    "session.state[\"metadata_store\"] = with_embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d79e7169-0766-4a9b-bb24-c46dd7541319",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from google.adk.runners import Runner\n",
    "from google.genai import types # For creating response content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97dd4ab1-0da7-4ddb-88e7-5faaeb6cb4a4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "runner = Runner(app_name = \"my_app\", agent=rag_agent, session_service=session_service)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef2e77a7-bde0-4d09-8ef0-13e213d10c07",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Create a user message\n",
    "content = types.Content(role='user', parts=[types.Part(text=\"What is media effectiveness\")])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c595b35-bea5-4040-a161-00b1ef2c9299",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Run the agent\n",
    "events = runner.run(user_id=\"user123\", session_id=session.id, new_message=content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85c3331d-93ac-49cd-92b9-d91f7e15ce34",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Process the events\n",
    "for event in events:\n",
    "    if event.is_final_response():\n",
    "        final_response = event.content.parts[0].text\n",
    "        print(\"Agent Response:\", final_response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e21226e1-1c27-4f9a-8697-c94d66806bf2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "c198dac1-9a27-49c5-8031-b24669b93624",
   "metadata": {},
   "source": [
    "#### 3. Evaluation framework\n",
    "- Code that we can use to evaluate the Agent \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08ec3b73-4424-44bd-a304-fd448053def5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "endpoint = aiplatform.MatchingEngineIndexEndpoint(\n",
    "        index_endpoint_name=\"7730008746939645952\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67a5faff-c05a-47b4-87d6-aa32873c76a1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "endpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a232b9be-2d90-4dc5-b045-6644bf01d0c0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "957ccdf9-1743-4777-a029-67d0dc7c0174",
   "metadata": {},
   "source": [
    "691974057.us-central1-163097687798.vdb.vertexai.goog"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19d8bb2c-978b-4d97-8b91-514bf4c7297e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "settings.GOOGLE_CLOUD_PROJECT, settings.VERTEX_AI_LOCATION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20504fb1-af8b-49ee-86b8-1972b27d4433",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "aiplatform.init(project=settings.GOOGLE_CLOUD_PROJECT, location=settings.VERTEX_AI_LOCATION)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eba8c48f-861e-4dbe-a0a4-d883e82730b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "result = endpoint.find_neighbors(\n",
    "    deployed_index_id=\"deployed_index_5428000834283634688_v1\",\n",
    "    queries=[[123]],\n",
    "    num_neighbors=3\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb7d072c-88ca-4514-bb34-26830410ee47",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "result[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbe212b6-bb9d-4710-b41a-65f7b54c0fd0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "endpoint.public_endpoint_domain_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8928ecd-7720-4d84-a497-87002b4108e5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "endpoint._public_match_client = '691974057.us-central1-163097687798.vdb.vertexai.goog'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b19f1880-25a6-4f08-85f0-c055d9a4051d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "endpoint._public_match_client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5c2b826-e1c5-4998-9469-3e49337d94cf",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38641d35-6f5a-450e-98eb-190a278dd2dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.cloud.aiplatform import agent_engines"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28f27d94-204f-4a42-a617-e4b196603e58",
   "metadata": {},
   "source": [
    "### test agent engine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45763f36-1b03-43a1-8c7f-deb05a6c1671",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import vertexai\n",
    "from vertexai import agent_engines\n",
    "\n",
    "vertexai.init(\n",
    "    project=settings.GOOGLE_CLOUD_PROJECT,               # Your project ID.\n",
    "    location=settings.VERTEX_AI_LOCATION,                # Your cloud region.\n",
    "    staging_bucket = \"gs://yuan_evernote_rag_cs\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e76117dd-1890-46c9-92b1-ebae78c2da46",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from google.adk.agents import Agent\n",
    "from vertexai.preview.reasoning_engines import AdkApp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4bac25d-818d-46db-abc1-53b25afbaf51",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from src.common import VectorStore, EmbeddingGenerator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d802b1e7-4e46-4a2a-a30b-969f5565a4ac",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Initialize shared utilities\n",
    "_vs = VectorStore()\n",
    "_embedder = EmbeddingGenerator()\n",
    "\n",
    "#@function_tool\n",
    "def retrieve_documents(question: str, top_k: int = 5) -> List[str]:\n",
    "    \"\"\"\n",
    "    Vector-search tool: returns the top_k text snippets relevant to the question.\n",
    "\n",
    "    Args:\n",
    "        question: User query text.\n",
    "        top_k: Number of similar chunks to return.\n",
    "    Returns:\n",
    "        List of document text snippets.\n",
    "    \"\"\"\n",
    "    query_emb = _embedder.generate_single_embedding(question)\n",
    "    hits = _vs.search_vectors(query_emb, top_k=top_k)\n",
    "    return [hit['metadata'].get('text', '') for hit in hits]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54f37ad1-091e-4cfd-be80-5542e5d868b5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "rag_agent = Agent(\n",
    "    name=\"cymbal_knowledge_agent\",\n",
    "    model=\"gemini-2.0-flash-001\",\n",
    "    #description=\"Company knowledge assistant that can ingest new files on demand and answer based on our internal documents.\",\n",
    "    #instruction=(\n",
    "    #    \"You are a corporate knowledge assistant. \"\n",
    "    #    \"For any question, always call `retrieve_documents(question)` first to fetch relevant context before answering.\"\n",
    "    #),\n",
    "    tools=[retrieve_documents],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1037d8a3-3198-4ed2-b22e-ed186cc2bc7d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import inspect\n",
    "\n",
    "methods = [name for name, member in inspect.getmembers(Agent, predicate=inspect.isfunction)]\n",
    "print(\"Instance/Static Methods:\", methods)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20b193d4-2377-4937-9958-45d50c91389f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "200f7804-9853-431d-bb24-ece78a79c516",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "response = rag_agent.query(\n",
    "    input=\"What is the exchange rate from US dollars to Swedish currency?\"\n",
    ")\n",
    "\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d0374d4-d85d-4794-949a-383c6ac1c743",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "app = AdkApp(agent=rag_agent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f853f63-7027-4b39-a96e-61aafaac35d0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from vertexai import agent_engines\n",
    "\n",
    "remote_agent = agent_engines.create(\n",
    "    app,\n",
    "    requirements=[\"google-cloud-aiplatform[agent_engines,adk]\"],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21892810-ca87-4350-9b6e-4192a04654bf",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def test_function():\n",
    "    return \"yes\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8430caf-44be-4d8b-8c2a-f8b4e6b55488",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from google.adk.agents import Agent\n",
    "from vertexai.preview.reasoning_engines import AdkApp\n",
    "\n",
    "agent = Agent(\n",
    "    model=\"gemini-2.0-flash\",\n",
    "    name='currency_exchange_agent',\n",
    "    tools=[test_function],\n",
    ")\n",
    "\n",
    "app = AdkApp(agent=agent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcbe7463-abd8-4302-ba13-282accdacc5d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "remote_agent = agent_engines.create(\n",
    "    agent,                    # Optional.\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1bd2fe5-33b7-480c-8d2a-faa9b1edb223",
   "metadata": {},
   "source": [
    "### Testing index endpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "2bfba040-d883-4638-8471-e82f6d8b875f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from __future__ import annotations\n",
    "import numpy as np\n",
    "from typing import List, Dict, Any, Optional\n",
    "from google.cloud import aiplatform\n",
    "from google.cloud.aiplatform.matching_engine import (\n",
    "    MatchingEngineIndex,\n",
    "    MatchingEngineIndexEndpoint,\n",
    ")\n",
    "from google.cloud.aiplatform_v1.types import IndexDatapoint\n",
    "from google.cloud import firestore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "da4c472a-cf86-49fa-bf92-03a107dd5c3e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "processor = DocumentProcessor()\n",
    "embedder = EmbeddingGenerator()\n",
    "db = firestore.Client()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2255237-a2b4-43f7-b27f-de0fe69a8fb8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# main process\n",
    "file_root = '/home/jupyter/code_test/adk_rag/prototype/test_data'\n",
    "\n",
    "# chunking \n",
    "processed_data = processor.process_document(file_root)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68b9b435-184f-4914-af02-3b57607c3688",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# convert to embeddings - upsert\n",
    "with_embeddings = embedder.generate_embeddings(processed_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "714f33bc-9118-4ce3-9550-5e2114a662c5",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<google.cloud.aiplatform.matching_engine.matching_engine_index.MatchingEngineIndex object at 0x7f43911c7f40> \n",
       " resource name: projects/163097687798/locations/us-central1/indexes/7922713552870178816]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "display_name = \"test_index\"\n",
    "matches = MatchingEngineIndex.list(\n",
    "    filter=f'display_name=\"{display_name}\"'\n",
    ")\n",
    "\n",
    "index = matches[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d4d63b8-674a-466e-aa5e-49580aeb0d45",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "# upsert vectors\n",
    "        # `upsert_datapoints` (Vertex AI 2.15+)\n",
    "# extract vectors \n",
    "datapoints = [IndexDatapoint(datapoint_id=str(i), feature_vector=e['embedding']) for i, e in enumerate(with_embeddings)]\n",
    "index.upsert_datapoints(\n",
    "    datapoints = datapoints\n",
    ")\n",
    "\"\"\"\n",
    "\"\"\"\n",
    "# Save to Firestore under 'files' collection\n",
    "for item in with_embeddings:\n",
    "    text = item['text']\n",
    "    metadata = item['metadata']\n",
    "    idx = metadata['chunk_index']\n",
    "    file_name = metadata['file_name']\n",
    "    source = metadata['source']\n",
    "    doc_ref = db.collection(\"rag\").document(str(idx))\n",
    "    doc_ref.set({\n",
    "        \"file_path\": source,\n",
    "        \"file_name\": file_name,\n",
    "        \"text\": item[\"text\"]\n",
    "    })\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34e3958a-21e2-476c-8a48-cdede79a1f0c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "endpoint_display_name = \"test_index_endpoint\"\n",
    "endpoint = MatchingEngineIndexEndpoint.create(\n",
    "        display_name=endpoint_display_name,\n",
    "        public_endpoint_enabled=True\n",
    "    )\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "fb85ef4c-26cc-47ab-a304-fbeb412eb448",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "endpoint = aiplatform.MatchingEngineIndexEndpoint(\n",
    "    index_endpoint_name=\"7694472531129925632\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a017ec22-c6f8-458d-b8d8-639fa6909a23",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<google.cloud.aiplatform.matching_engine.matching_engine_index.MatchingEngineIndex object at 0x7f43910d7730> \n",
       " resource name: projects/163097687798/locations/us-central1/indexes/7922713552870178816]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "MatchingEngineIndex.list(\n",
    "            filter=f'display_name=\"{display_name}\"'\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "37d82287-b30e-47a1-9b5a-c5e5e6ebc1c0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "query = \"media effectiveness\"\n",
    "query_embedding = embedder.generate_single_embedding(query)\n",
    "response = endpoint.find_neighbors(\n",
    "    deployed_index_id=\"deployed_index_1747401318896\",\n",
    "    queries=[query_embedding],\n",
    "    num_neighbors=3,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "cb93aef2-b02d-434c-b1bd-aeb34d64bd9c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "retrieved_results = []\n",
    "for response_ in response[0]:\n",
    "    r = response_.id\n",
    "    r = db.collection(\"rag\").document(r).get().to_dict()\n",
    "    retrieved_results.append(\n",
    "        {\n",
    "            'text': r['text'],\n",
    "            'file_name': r['file_name'],\n",
    "            'file_path': r['file_path']\n",
    "        }\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "67132774-9988-4e22-9db9-b77b0a6db606",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'text': 'ul\\nMasterChef\\n\\nMEDIA MEASUREMENT',\n",
       "  'file_name': 'ChatGPT Image Apr 30, 2025, 12_07_00 PM.png',\n",
       "  'file_path': '/home/jupyter/code_test/adk_rag/prototype/test_data/ChatGPT Image Apr 30, 2025, 12_07_00 PM.png'},\n",
       " {'text': 'name: LY | email: 123@gmail.com\\n name: YY | email: 234@yahoo.com\\n name: TJ | email: 345@ntu.com.sg',\n",
       "  'file_name': 'email.csv',\n",
       "  'file_path': '/home/jupyter/code_test/adk_rag/prototype/test_data/email.csv'},\n",
       " {'text': 'Large language models are powerful tools with extensive capabilities; nonetheless, they grapple with a distinct limitation known as the context window. This context window defines the boundaries within which these models can proficiently process text. Take, for example, gpt-3.5-turbo, which operates within a context length of 4,096 tokens, approximately corresponding to 3,500 words.\\n\\nBut what occurs when you present these models with a document that exceeds their context window? This is where a clever strategy known as \"chunking\" comes into play. Chunking involves dividing the document into smaller, more manageable sections that fit comfortably within the context window of the large language model.\\n\\nLangchain provides users with a range of chunking techniques to choose from. However, among these options, the RecursiveCharacterTextSplitter emerges as the favored and strongly recommended method.',\n",
       "  'file_name': 'test.txt',\n",
       "  'file_path': '/home/jupyter/code_test/adk_rag/prototype/test_data/test.txt'}]"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "retrieved_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "818c860e-c9a5-4f05-b893-9eac69708439",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "environment": {
   "kernel": "myenv",
   "name": "tf2-gpu.2-17.m128",
   "type": "gcloud",
   "uri": "us-docker.pkg.dev/deeplearning-platform-release/gcr.io/tf2-gpu.2-17:m128"
  },
  "kernelspec": {
   "display_name": "Python (myenv) (Local)",
   "language": "python",
   "name": "myenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
